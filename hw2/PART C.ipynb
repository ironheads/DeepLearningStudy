{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Homework 2 Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the torch cuda is ok\n",
    "import torch\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1'\n",
    "torch.cuda.is_available()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "def create_model(args):\n",
    "    model = models.resnet18(pretrained=False)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc=nn.Linear(in_features,args.num_classes)\n",
    "    model.name = \"resnet18\"\n",
    "    print(args.device)\n",
    "    model.to(args.device)\n",
    "    criteria_x = nn.CrossEntropyLoss().to(args.device)\n",
    "    criteria_u = nn.CrossEntropyLoss(reduction='none').to(args.device)\n",
    "    return model,criteria_x,criteria_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(gpu_id=1,\n",
       "          num_workers=4,\n",
       "          dataset='dataset',\n",
       "          num_labeled=250,\n",
       "          expand_labels=True,\n",
       "          arch='resnet',\n",
       "          total_steps=1048576,\n",
       "          eval_steps=1024,\n",
       "          start_epoch=0,\n",
       "          batch_size=16,\n",
       "          lr=0.03,\n",
       "          warmu=0,\n",
       "          wdecay=0.0005,\n",
       "          momentum=0.9,\n",
       "          nesterov=True,\n",
       "          use_ema=True,\n",
       "          ema_decay=0.999,\n",
       "          mu=7,\n",
       "          lambda_u=1,\n",
       "          T=1,\n",
       "          threshold=0.95,\n",
       "          out='result',\n",
       "          resume='resume',\n",
       "          seed=None,\n",
       "          local_rank=-1,\n",
       "          no_progress=False,\n",
       "          input_size=224,\n",
       "          num_classes=10,\n",
       "          num_images_per_epoch=16384,\n",
       "          num_epoches=128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types \n",
    "args=types.SimpleNamespace()\n",
    "\n",
    "args.gpu_id = 1\n",
    "args.num_workers = 4\n",
    "args.dataset = 'dataset'\n",
    "args.num_labeled = 250\n",
    "args.expand_labels = True\n",
    "args.arch = 'resnet'\n",
    "args.total_steps = 2**20\n",
    "args.eval_steps = 1024\n",
    "args.start_epoch = 0\n",
    "args.batch_size = 16\n",
    "args.lr = 0.03\n",
    "args.warmu = 0\n",
    "args.wdecay = 5e-4\n",
    "args.momentum = 0.9\n",
    "args.nesterov = True\n",
    "args.use_ema = True\n",
    "args.ema_decay = 0.999\n",
    "args.mu = 7\n",
    "args.lambda_u = 1\n",
    "args.T = 1\n",
    "args.threshold = 0.95 \n",
    "args.out = 'result'\n",
    "args.resume = 'resume'\n",
    "args.seed = None\n",
    "# args.amp = True\n",
    "# args.opt_level = '01'\n",
    "args.local_rank = -1\n",
    "args.no_progress = False\n",
    "args.input_size = 224\n",
    "args.num_classes = 10\n",
    "args.num_images_per_epoch = 2**14\n",
    "args.num_epoches = 128\n",
    "\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.local_rank == -1:\n",
    "    device = torch.device('cuda', args.gpu_id)\n",
    "    args.world_size = 1\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device('cuda', args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.world_size = torch.distributed.get_world_size()\n",
    "    args.n_gpu = 1\n",
    "\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## aug functions\n",
    "def identity_func(img):\n",
    "    return img\n",
    "\n",
    "\n",
    "def autocontrast_func(img, cutoff=0):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.autocontrast\n",
    "    '''\n",
    "    n_bins = 256\n",
    "\n",
    "    def tune_channel(ch):\n",
    "        n = ch.size\n",
    "        cut = cutoff * n // 100\n",
    "        if cut == 0:\n",
    "            high, low = ch.max(), ch.min()\n",
    "        else:\n",
    "            hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n",
    "            low = np.argwhere(np.cumsum(hist) > cut)\n",
    "            low = 0 if low.shape[0] == 0 else low[0]\n",
    "            high = np.argwhere(np.cumsum(hist[::-1]) > cut)\n",
    "            high = n_bins - 1 if high.shape[0] == 0 else n_bins - 1 - high[0]\n",
    "        if high <= low:\n",
    "            table = np.arange(n_bins)\n",
    "        else:\n",
    "            scale = (n_bins - 1) / (high - low)\n",
    "            offset = -low * scale\n",
    "            table = np.arange(n_bins) * scale + offset\n",
    "            table[table < 0] = 0\n",
    "            table[table > n_bins - 1] = n_bins - 1\n",
    "        table = table.clip(0, 255).astype(np.uint8)\n",
    "        return table[ch]\n",
    "\n",
    "    channels = [tune_channel(ch) for ch in cv2.split(img)]\n",
    "    out = cv2.merge(channels)\n",
    "    return out\n",
    "\n",
    "\n",
    "def equalize_func(img):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.equalize\n",
    "        PIL's implementation is different from cv2.equalize\n",
    "    '''\n",
    "    n_bins = 256\n",
    "\n",
    "    def tune_channel(ch):\n",
    "        hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n",
    "        non_zero_hist = hist[hist != 0].reshape(-1)\n",
    "        step = np.sum(non_zero_hist[:-1]) // (n_bins - 1)\n",
    "        if step == 0: return ch\n",
    "        n = np.empty_like(hist)\n",
    "        n[0] = step // 2\n",
    "        n[1:] = hist[:-1]\n",
    "        table = (np.cumsum(n) // step).clip(0, 255).astype(np.uint8)\n",
    "        return table[ch]\n",
    "\n",
    "    channels = [tune_channel(ch) for ch in cv2.split(img)]\n",
    "    out = cv2.merge(channels)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rotate_func(img, degree, fill=(0, 0, 0)):\n",
    "    '''\n",
    "    like PIL, rotate by degree, not radians\n",
    "    '''\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    center = W / 2, H / 2\n",
    "    M = cv2.getRotationMatrix2D(center, degree, 1)\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill)\n",
    "    return out\n",
    "\n",
    "\n",
    "def solarize_func(img, thresh=128):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.posterize\n",
    "    '''\n",
    "    table = np.array([el if el < thresh else 255 - el for el in range(256)])\n",
    "    table = table.clip(0, 255).astype(np.uint8)\n",
    "    out = table[img]\n",
    "    return out\n",
    "\n",
    "\n",
    "def color_func(img, factor):\n",
    "    '''\n",
    "        same output as PIL.ImageEnhance.Color\n",
    "    '''\n",
    "    ## implementation according to PIL definition, quite slow\n",
    "    #  degenerate = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[:, :, np.newaxis]\n",
    "    #  out = blend(degenerate, img, factor)\n",
    "    #  M = (\n",
    "    #      np.eye(3) * factor\n",
    "    #      + np.float32([0.114, 0.587, 0.299]).reshape(3, 1) * (1. - factor)\n",
    "    #  )[np.newaxis, np.newaxis, :]\n",
    "    M = (\n",
    "            np.float32([\n",
    "                [0.886, -0.114, -0.114],\n",
    "                [-0.587, 0.413, -0.587],\n",
    "                [-0.299, -0.299, 0.701]]) * factor\n",
    "            + np.float32([[0.114], [0.587], [0.299]])\n",
    "    )\n",
    "    out = np.matmul(img, M).clip(0, 255).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def contrast_func(img, factor):\n",
    "    \"\"\"\n",
    "        same output as PIL.ImageEnhance.Contrast\n",
    "    \"\"\"\n",
    "    mean = np.sum(np.mean(img, axis=(0, 1)) * np.array([0.114, 0.587, 0.299]))\n",
    "    table = np.array([(\n",
    "        el - mean) * factor + mean\n",
    "        for el in range(256)\n",
    "    ]).clip(0, 255).astype(np.uint8)\n",
    "    out = table[img]\n",
    "    return out\n",
    "\n",
    "\n",
    "def brightness_func(img, factor):\n",
    "    '''\n",
    "        same output as PIL.ImageEnhance.Contrast\n",
    "    '''\n",
    "    table = (np.arange(256, dtype=np.float32) * factor).clip(0, 255).astype(np.uint8)\n",
    "    out = table[img]\n",
    "    return out\n",
    "\n",
    "\n",
    "def sharpness_func(img, factor):\n",
    "    '''\n",
    "    The differences the this result and PIL are all on the 4 boundaries, the center\n",
    "    areas are same\n",
    "    '''\n",
    "    kernel = np.ones((3, 3), dtype=np.float32)\n",
    "    kernel[1][1] = 5\n",
    "    kernel /= 13\n",
    "    degenerate = cv2.filter2D(img, -1, kernel)\n",
    "    if factor == 0.0:\n",
    "        out = degenerate\n",
    "    elif factor == 1.0:\n",
    "        out = img\n",
    "    else:\n",
    "        out = img.astype(np.float32)\n",
    "        degenerate = degenerate.astype(np.float32)[1:-1, 1:-1, :]\n",
    "        out[1:-1, 1:-1, :] = degenerate + factor * (out[1:-1, 1:-1, :] - degenerate)\n",
    "        out = out.astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def shear_x_func(img, factor, fill=(0, 0, 0)):\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, factor, 0], [0, 1, 0]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def translate_x_func(img, offset, fill=(0, 0, 0)):\n",
    "    '''\n",
    "        same output as PIL.Image.transform\n",
    "    '''\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, 0, -offset], [0, 1, 0]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def translate_y_func(img, offset, fill=(0, 0, 0)):\n",
    "    '''\n",
    "        same output as PIL.Image.transform\n",
    "    '''\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, 0, 0], [0, 1, -offset]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def posterize_func(img, bits):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.posterize\n",
    "    '''\n",
    "    out = np.bitwise_and(img, np.uint8(255 << (8 - bits)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def shear_y_func(img, factor, fill=(0, 0, 0)):\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, 0, 0], [factor, 1, 0]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cutout_func(img, pad_size, replace=(0, 0, 0)):\n",
    "    replace = np.array(replace, dtype=np.uint8)\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    rh, rw = np.random.random(2)\n",
    "    pad_size = pad_size // 2\n",
    "    ch, cw = int(rh * H), int(rw * W)\n",
    "    x1, x2 = max(ch - pad_size, 0), min(ch + pad_size, H)\n",
    "    y1, y2 = max(cw - pad_size, 0), min(cw + pad_size, W)\n",
    "    out = img.copy()\n",
    "    out[x1:x2, y1:y2, :] = replace\n",
    "    return out\n",
    "\n",
    "\n",
    "### level to args\n",
    "def enhance_level_to_args(MAX_LEVEL):\n",
    "    def level_to_args(level):\n",
    "        return ((level / MAX_LEVEL) * 1.8 + 0.1,)\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def shear_level_to_args(MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = (level / MAX_LEVEL) * 0.3\n",
    "        if np.random.random() > 0.5: level = -level\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def translate_level_to_args(translate_const, MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = (level / MAX_LEVEL) * float(translate_const)\n",
    "        if np.random.random() > 0.5: level = -level\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def cutout_level_to_args(cutout_const, MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = int((level / MAX_LEVEL) * cutout_const)\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def solarize_level_to_args(MAX_LEVEL):\n",
    "    def level_to_args(level):\n",
    "        level = int((level / MAX_LEVEL) * 256)\n",
    "        return (level, )\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def none_level_to_args(level):\n",
    "    return ()\n",
    "\n",
    "\n",
    "def posterize_level_to_args(MAX_LEVEL):\n",
    "    def level_to_args(level):\n",
    "        level = int((level / MAX_LEVEL) * 4)\n",
    "        return (level, )\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def rotate_level_to_args(MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = (level / MAX_LEVEL) * 30\n",
    "        if np.random.random() < 0.5:\n",
    "            level = -level\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "func_dict = {\n",
    "    'Identity': identity_func,\n",
    "    'AutoContrast': autocontrast_func,\n",
    "    'Equalize': equalize_func,\n",
    "    'Rotate': rotate_func,\n",
    "    'Solarize': solarize_func,\n",
    "    'Color': color_func,\n",
    "    'Contrast': contrast_func,\n",
    "    'Brightness': brightness_func,\n",
    "    'Sharpness': sharpness_func,\n",
    "    'ShearX': shear_x_func,\n",
    "    'TranslateX': translate_x_func,\n",
    "    'TranslateY': translate_y_func,\n",
    "    'Posterize': posterize_func,\n",
    "    'ShearY': shear_y_func,\n",
    "}\n",
    "\n",
    "translate_const = 10\n",
    "MAX_LEVEL = 10\n",
    "replace_value = (128, 128, 128)\n",
    "arg_dict = {\n",
    "    'Identity': none_level_to_args,\n",
    "    'AutoContrast': none_level_to_args,\n",
    "    'Equalize': none_level_to_args,\n",
    "    'Rotate': rotate_level_to_args(MAX_LEVEL, replace_value),\n",
    "    'Solarize': solarize_level_to_args(MAX_LEVEL),\n",
    "    'Color': enhance_level_to_args(MAX_LEVEL),\n",
    "    'Contrast': enhance_level_to_args(MAX_LEVEL),\n",
    "    'Brightness': enhance_level_to_args(MAX_LEVEL),\n",
    "    'Sharpness': enhance_level_to_args(MAX_LEVEL),\n",
    "    'ShearX': shear_level_to_args(MAX_LEVEL, replace_value),\n",
    "    'TranslateX': translate_level_to_args(\n",
    "        translate_const, MAX_LEVEL, replace_value\n",
    "    ),\n",
    "    'TranslateY': translate_level_to_args(\n",
    "        translate_const, MAX_LEVEL, replace_value\n",
    "    ),\n",
    "    'Posterize': posterize_level_to_args(MAX_LEVEL),\n",
    "    'ShearY': shear_level_to_args(MAX_LEVEL, replace_value),\n",
    "}\n",
    "\n",
    "\n",
    "class RandomAugment(object):\n",
    "\n",
    "    def __init__(self, N=2, M=10):\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "\n",
    "    def get_random_ops(self):\n",
    "        sampled_ops = np.random.choice(list(func_dict.keys()), self.N)\n",
    "        return [(op, 0.5, self.M) for op in sampled_ops]\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        ops = self.get_random_ops()\n",
    "        for name, prob, level in ops:\n",
    "            if np.random.random() > prob:\n",
    "                continue\n",
    "            args = arg_dict[name](level)\n",
    "            img = func_dict[name](img, *args)\n",
    "        img = cutout_func(img, 16, replace_value)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self,dataset:Dataset,args,is_train=True):\n",
    "        super().__init__()\n",
    "        self.dataset=dataset\n",
    "        self.is_train=is_train\n",
    "        self.input_size = args.input_size\n",
    "        if self.is_train:\n",
    "            self.trans_weak = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            self.trans_strong = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                RandomAugment(2,10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    def __getitem__(self,index):\n",
    "        image,label = self.dataset[index]\n",
    "        # image=np.array(image)\n",
    "        # print(image.shape)\n",
    "        if self.is_train:\n",
    "            return self.trans_weak(image),self.trans_strong(image),label\n",
    "        else:\n",
    "            return self.trans(image),label\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self,image_dir,args,is_train=True) ->None:\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.image_name =os.listdir(self.image_dir)\n",
    "        self.is_train = is_train\n",
    "        self.input_size = args.input_size\n",
    "        if self.is_train:\n",
    "            self.trans_weak = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "            self.trans_strong = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                RandomAugment(2,10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.trans = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.input_size),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        image_item_name = self.image_name[index]\n",
    "        image_item_path = os.path.join(self.image_dir,image_item_name)\n",
    "        image = Image.open(image_item_path)\n",
    "        # image=np.array(image)\n",
    "        label = 'unlabeled'\n",
    "        if self.is_train:\n",
    "            return self.trans_weak(image),self.trans_strong(image),label\n",
    "        else:\n",
    "            return self.trans(image),label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "\"\"\"\n",
    "为什么 param 和 buffer 要采用不同的的更新策略\n",
    "param 是 指数移动平均数，buffer 不是\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class EMA(object):\n",
    "    def __init__(self, model, alpha=0.999):\n",
    "        self.step = 0\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.shadow = self.get_model_state()\n",
    "        self.backup = {}\n",
    "        self.param_keys = [k for k, _ in self.model.named_parameters()]\n",
    "        # num_batches_tracked, running_mean, running_var in bn\n",
    "        self.buffer_keys = [k for k, _ in self.model.named_buffers()]\n",
    "\n",
    "    def update_params(self):\n",
    "        # decay = min(self.alpha, (self.step + 1) / (self.step + 10))  # ????\n",
    "        decay = self.alpha\n",
    "        state = self.model.state_dict()  # current params\n",
    "        for name in self.param_keys:\n",
    "            self.shadow[name].copy_(\n",
    "                decay * self.shadow[name] + (1 - decay) * state[name]\n",
    "            )\n",
    "        # for name in self.buffer_keys:\n",
    "        #     self.shadow[name].copy_(\n",
    "        #         decay * self.shadow[name]\n",
    "        #         + (1 - decay) * state[name]\n",
    "        #     )\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "    def update_buffer(self):\n",
    "        # without EMA\n",
    "        state = self.model.state_dict()\n",
    "        for name in self.buffer_keys:\n",
    "            self.shadow[name].copy_(state[name])\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        self.backup = self.get_model_state()\n",
    "        self.model.load_state_dict(self.shadow)\n",
    "\n",
    "    def restore(self):\n",
    "        self.model.load_state_dict(self.backup)\n",
    "\n",
    "    def get_model_state(self):\n",
    "        return {\n",
    "            k: v.clone().detach()\n",
    "            for k, v in self.model.state_dict().items()\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, BatchSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def get_train_loader(args):\n",
    "    num_iters_per_epoch = args.num_images_per_epoch // args.batch_size\n",
    "    datasetPath = os.path.join(args.dataset,'3-Semi-Supervised')\n",
    "    labeledDatasetPath = os.path.join(datasetPath,'labeled')\n",
    "    unlabeledDatasetPath = os.path.join(datasetPath,'unlabeled')\n",
    "    labeledDataset = datasets.ImageFolder(labeledDatasetPath)\n",
    "    unlabeledDataset = UnlabeledDataset(unlabeledDatasetPath,args)\n",
    "    labeledDataset = LabeledDataset(labeledDataset,args)\n",
    "    sampler_x = RandomSampler(labeledDataset, replacement=True, num_samples=num_iters_per_epoch * args.batch_size)\n",
    "    batch_sampler_x = BatchSampler(sampler_x, args.batch_size, drop_last=True)  # yield a batch of samples one time\n",
    "    labeledDatasetDataloader = DataLoader(\n",
    "        labeledDataset,\n",
    "        batch_sampler=batch_sampler_x,\n",
    "        num_workers = args.num_workers\n",
    "    )\n",
    "    sampler_u = RandomSampler(unlabeledDataset, replacement=True, num_samples=args.mu * num_iters_per_epoch * args.batch_size)\n",
    "    batch_sampler_u = BatchSampler(sampler_u, args.batch_size * args.mu, drop_last=True)\n",
    "    unlabeledDatasetDataloader = DataLoader(\n",
    "        unlabeledDataset,\n",
    "        batch_sampler=batch_sampler_u,\n",
    "        num_workers = args.num_workers\n",
    "    )\n",
    "    return labeledDatasetDataloader,unlabeledDatasetDataloader\n",
    "\n",
    "\n",
    "def get_valid_loader(args):\n",
    "    testDataset = datasets.ImageFolder(os.path.join(args.dataset,'test'))\n",
    "    testDataset = LabeledDataset(testDataset,args,is_train=False)\n",
    "    validDatasetDataloader = DataLoader(\n",
    "        testDataset,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        batch_size = 64,\n",
    "        num_workers = args.num_workers\n",
    "    )\n",
    "    return validDatasetDataloader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler, LambdaLR\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WarmupExpLrScheduler(_LRScheduler):\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer,\n",
    "            power,\n",
    "            step_interval=1,\n",
    "            warmup_iter=500,\n",
    "            warmup_ratio=5e-4,\n",
    "            warmup='exp',\n",
    "            last_epoch=-1,\n",
    "    ):\n",
    "        self.power = power\n",
    "        self.step_interval = step_interval\n",
    "        self.warmup_iter = warmup_iter\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.warmup = warmup\n",
    "        super(WarmupExpLrScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        ratio = self.get_lr_ratio()\n",
    "        lrs = [ratio * lr for lr in self.base_lrs]\n",
    "        return lrs\n",
    "\n",
    "    def get_lr_ratio(self):\n",
    "        if self.last_epoch < self.warmup_iter:\n",
    "            ratio = self.get_warmup_ratio()\n",
    "        else:\n",
    "            real_iter = self.last_epoch - self.warmup_iter\n",
    "            ratio = self.power ** (real_iter // self.step_interval)\n",
    "        return ratio\n",
    "\n",
    "    def get_warmup_ratio(self):\n",
    "        assert self.warmup in ('linear', 'exp')\n",
    "        alpha = self.last_epoch / self.warmup_iter\n",
    "        if self.warmup == 'linear':\n",
    "            ratio = self.warmup_ratio + (1 - self.warmup_ratio) * alpha\n",
    "        elif self.warmup == 'exp':\n",
    "            ratio = self.warmup_ratio ** (1. - alpha)\n",
    "        return ratio\n",
    "\n",
    "\n",
    "class WarmupPolyLrScheduler(_LRScheduler):\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer,\n",
    "            power,\n",
    "            max_iter,\n",
    "            warmup_iter,\n",
    "            warmup_ratio=5e-4,\n",
    "            warmup='exp',\n",
    "            last_epoch=-1,\n",
    "    ):\n",
    "        self.power = power\n",
    "        self.max_iter = max_iter\n",
    "        self.warmup_iter = warmup_iter\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.warmup = warmup\n",
    "        super(WarmupPolyLrScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        ratio = self.get_lr_ratio()\n",
    "        lrs = [ratio * lr for lr in self.base_lrs]\n",
    "        return lrs\n",
    "\n",
    "    def get_lr_ratio(self):\n",
    "        if self.last_epoch < self.warmup_iter:\n",
    "            ratio = self.get_warmup_ratio()\n",
    "        else:\n",
    "            real_iter = self.last_epoch - self.warmup_iter\n",
    "            real_max_iter = self.max_iter - self.warmup_iter\n",
    "            alpha = real_iter / real_max_iter\n",
    "            ratio = (1 - alpha) ** self.power\n",
    "        return ratio\n",
    "\n",
    "    def get_warmup_ratio(self):\n",
    "        assert self.warmup in ('linear', 'exp')\n",
    "        alpha = self.last_epoch / self.warmup_iter\n",
    "        if self.warmup == 'linear':\n",
    "            ratio = self.warmup_ratio + (1 - self.warmup_ratio) * alpha\n",
    "        elif self.warmup == 'exp':\n",
    "            ratio = self.warmup_ratio ** (1. - alpha)\n",
    "        return ratio\n",
    "\n",
    "\n",
    "class WarmupCosineLrScheduler(_LRScheduler):\n",
    "    '''\n",
    "    This is different from official definition, this is implemented according to\n",
    "    the paper of fix-match\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            optimizer,\n",
    "            max_iter,\n",
    "            warmup_iter,\n",
    "            warmup_ratio=5e-4,\n",
    "            warmup='exp',\n",
    "            last_epoch=-1,\n",
    "    ):\n",
    "        self.max_iter = max_iter\n",
    "        self.warmup_iter = warmup_iter\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.warmup = warmup\n",
    "        super(WarmupCosineLrScheduler, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        ratio = self.get_lr_ratio()\n",
    "        lrs = [ratio * lr for lr in self.base_lrs]\n",
    "        return lrs\n",
    "\n",
    "    def get_lr_ratio(self):\n",
    "        if self.last_epoch < self.warmup_iter:\n",
    "            ratio = self.get_warmup_ratio()\n",
    "        else:\n",
    "            real_iter = self.last_epoch - self.warmup_iter\n",
    "            real_max_iter = self.max_iter - self.warmup_iter\n",
    "            ratio = np.cos((7 * np.pi * real_iter) / (16 * real_max_iter))\n",
    "        return ratio\n",
    "\n",
    "    def get_warmup_ratio(self):\n",
    "        assert self.warmup in ('linear', 'exp')\n",
    "        alpha = self.last_epoch / self.warmup_iter\n",
    "        if self.warmup == 'linear':\n",
    "            ratio = self.warmup_ratio + (1 - self.warmup_ratio) * alpha\n",
    "        elif self.warmup == 'exp':\n",
    "            ratio = self.warmup_ratio ** (1. - alpha)\n",
    "        return ratio\n",
    "\n",
    "\n",
    "# from Fixmatch-pytorch\n",
    "def get_cosine_schedule_with_warmup(optimizer,\n",
    "                                    num_warmup_steps,\n",
    "                                    num_training_steps,\n",
    "                                    num_cycles=7./16.,\n",
    "                                    last_epoch=-1):\n",
    "    def _lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        no_progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        # return max(0., math.cos(math.pi * num_cycles * no_progress))\n",
    "\n",
    "        return max(0., (math.cos(math.pi * num_cycles * no_progress) + 1) * 0.5)\n",
    "\n",
    "    return LambdaLR(optimizer, _lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def interleave(x, bt):\n",
    "    s = list(x.shape)\n",
    "    return torch.reshape(torch.transpose(x.reshape([-1, bt] + s[1:]), 1, 0), [-1] + s[1:])\n",
    "\n",
    "\n",
    "def de_interleave(x, bt):\n",
    "    s = list(x.shape)\n",
    "    return torch.reshape(torch.transpose(x.reshape([bt, -1] + s[1:]), 1, 0), [-1] + s[1:])\n",
    "\n",
    "\n",
    "def setup_default_logging(args, default_level=logging.INFO,\n",
    "                          format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"):\n",
    "    output_dir = os.path.join(args.dataset, f'x{args.num_labeled}')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    writer = SummaryWriter(comment=f'{args.dataset}_{args.num_labeled}')\n",
    "\n",
    "    logger = logging.getLogger('train')\n",
    "\n",
    "    logging.basicConfig(  # unlike the root logger, a custom logger can’t be configured using basicConfig()\n",
    "        filename=os.path.join(output_dir, f'{time_str()}.log'),\n",
    "        format=format,\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=default_level)\n",
    "\n",
    "    # print\n",
    "    # file_handler = logging.FileHandler()\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(default_level)\n",
    "    console_handler.setFormatter(logging.Formatter(format))\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    return logger, writer\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    \n",
    "    _, pred = output.topk(maxk, 1, largest=True, sorted=True)  # return value, indices\n",
    "    # print(pred)\n",
    "    pred = pred.t()\n",
    "    # print(target)\n",
    "    # print(target.view(1, -1).expand_as(pred))\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    # print(correct)\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        # self.avg = self.sum / (self.count + 1e-20)\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def time_str(fmt=None):\n",
    "    if fmt is None:\n",
    "        fmt = '%Y-%m-%d_%H:%M:%S'\n",
    "\n",
    "    #     time.strftime(format[, t])\n",
    "    return datetime.today().strftime(fmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    epoch,\n",
    "    model,\n",
    "    criteria_x,\n",
    "    criteria_u,\n",
    "    optimizer,\n",
    "    lr_schdlr,\n",
    "    ema,\n",
    "    labeledDatasetLoader,\n",
    "    unlabeledDatasetLoader,\n",
    "    lambda_u,\n",
    "    n_iters,\n",
    "    args,\n",
    "    logger\n",
    "    ):\n",
    "    model.train(True)\n",
    "    loss_meter = AverageMeter()\n",
    "    loss_x_meter = AverageMeter()\n",
    "    loss_u_meter = AverageMeter()\n",
    "    # number of gradient-considered strong augmentation of unlabeled samples\n",
    "    n_strong_aug_meter = AverageMeter()\n",
    "    mask_meter = AverageMeter()\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    labeledIter,unlabeledIter=iter(labeledDatasetLoader),iter(unlabeledDatasetLoader)\n",
    "    for it in range(n_iters):\n",
    "        image_labeled_weak ,image_labeled_strong, labeled_label = next(labeledIter)\n",
    "        image_unlabeled_weak,image_unlabeled_strong,_ =next(unlabeledIter)\n",
    "        labeled_label = labeled_label.to(args.device)\n",
    "        # print(labeled_label)\n",
    "        batch_size = image_labeled_weak.size(0)\n",
    "        mu = int(image_unlabeled_weak.size(0)//batch_size)\n",
    "        imgs = torch.cat([image_labeled_weak,image_unlabeled_weak,image_unlabeled_strong],dim=0).to(args.device)\n",
    "        imgs = interleave(imgs,2*mu+1)\n",
    "        logits = model(imgs)\n",
    "        logits = de_interleave(logits,2*mu+1)\n",
    "        logits_x = logits[:batch_size]\n",
    "        logits_unlabeled_w,logits_unlabeled_s = torch.split(logits[batch_size:],batch_size*mu)\n",
    "        loss_x = criteria_x(logits_x,labeled_label)\n",
    "        with torch.no_grad():\n",
    "            probs = torch.softmax(logits_unlabeled_w,dim=1)\n",
    "            scores,labels_unlabeled_guess = torch.max(probs,dim=1)\n",
    "            mask =scores.ge(args.threshold).float()\n",
    "        loss_u = (criteria_u(logits_unlabeled_s,labels_unlabeled_guess)*mask).mean()\n",
    "        loss = loss_x+ lambda_u *loss_u\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update_params()\n",
    "        lr_schdlr.step()\n",
    "        loss_meter.update(loss.item())\n",
    "        loss_x_meter.update(loss_x.item())\n",
    "        loss_u_meter.update(loss_u.item())\n",
    "        mask_meter.update(mask.mean().item())\n",
    "        n_strong_aug_meter.update(mask.sum().item())\n",
    "        if (it + 1) % 128 == 0:\n",
    "            t = time.time() - epoch_start\n",
    "            lr_log = [pg['lr'] for pg in optimizer.param_groups]\n",
    "            lr_log = sum(lr_log) / len(lr_log)\n",
    "\n",
    "            logger.info(\"epoch:{}, iter: {}. loss: {:.4f}. loss_u: {:.4f}. loss_x: {:.4f}. \"\n",
    "                        \"Mask:{:.4f} . LR: {:.4f}. Time: {:.2f}\".format(\n",
    "                epoch, it + 1, loss_meter.avg, loss_u_meter.avg, loss_x_meter.avg, n_strong_aug_meter.avg, mask_meter.avg, lr_log, t))\n",
    "\n",
    "            epoch_start = time.time()\n",
    "    ema.update_buffer()\n",
    "    return loss_meter.avg,loss_x_meter.avg,loss_u_meter.avg,mask_meter.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def evaluate(ema,dataloader,criterion,args):\n",
    "    # ema params to evaluate performance\n",
    "    ema.apply_shadow()\n",
    "    ema.model.to(args.device)\n",
    "    ema.model.eval()\n",
    "\n",
    "    loss_meter= AverageMeter()\n",
    "    top1_meter = AverageMeter()\n",
    "    top5_meter = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images,labels in dataloader:\n",
    "            images = images.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            logits = ema.model(images)\n",
    "            # print(images.shape,labels.shape)\n",
    "            loss = criterion(logits,labels)\n",
    "            scores = torch.softmax(logits,dim=1)\n",
    "            top1,top5 = accuracy(scores,labels,(1,5))\n",
    "            loss_meter.update(loss.item())\n",
    "            top1_meter.update(top1.item())\n",
    "            top5_meter.update(top5.item())\n",
    "\n",
    "    ema.restore()\n",
    "    return top1_meter.avg,top5_meter.avg,loss_meter.avg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "2022-04-12 07:36:34,344 - INFO - train -   ***** Running training *****\n",
      "2022-04-12 07:36:34,345 - INFO - train -     Task = dataset@250\n",
      "2022-04-12 07:36:34,346 - INFO - train -     Num Epochs = 1024\n",
      "2022-04-12 07:36:34,347 - INFO - train -     Batch size per GPU = 16\n",
      "2022-04-12 07:36:34,348 - INFO - train -     Total optimization steps = 131072\n",
      "2022-04-12 07:36:34,349 - INFO - train -   Total params: 11.18M\n",
      "2022-04-12 07:36:34,350 - INFO - train -   -----------start training--------------\n",
      "2022-04-12 07:37:23,278 - INFO - train -   epoch:0, iter: 128. loss: 1.9111. loss_u: 0.0351. loss_x: 1.8759. Mask:4.2500 . LR: 0.0379. Time: 0.03\n",
      "2022-04-12 07:38:11,113 - INFO - train -   epoch:0, iter: 256. loss: 1.7505. loss_u: 0.0572. loss_x: 1.6934. Mask:3.9297 . LR: 0.0351. Time: 0.03\n",
      "2022-04-12 07:38:59,678 - INFO - train -   epoch:0, iter: 384. loss: 1.6410. loss_u: 0.0640. loss_x: 1.5770. Mask:4.4922 . LR: 0.0401. Time: 0.03\n",
      "2022-04-12 07:39:48,019 - INFO - train -   epoch:0, iter: 512. loss: 1.5561. loss_u: 0.0817. loss_x: 1.4744. Mask:4.9531 . LR: 0.0442. Time: 0.03\n",
      "2022-04-12 07:40:36,291 - INFO - train -   epoch:0, iter: 640. loss: 1.4809. loss_u: 0.0823. loss_x: 1.3986. Mask:5.7031 . LR: 0.0509. Time: 0.03\n",
      "2022-04-12 07:41:24,223 - INFO - train -   epoch:0, iter: 768. loss: 1.4247. loss_u: 0.0892. loss_x: 1.3355. Mask:6.4219 . LR: 0.0573. Time: 0.03\n",
      "2022-04-12 07:42:12,410 - INFO - train -   epoch:0, iter: 896. loss: 1.3729. loss_u: 0.0987. loss_x: 1.2741. Mask:7.5469 . LR: 0.0674. Time: 0.03\n",
      "2022-04-12 07:43:00,350 - INFO - train -   epoch:0, iter: 1024. loss: 1.3272. loss_u: 0.1087. loss_x: 1.2185. Mask:8.5781 . LR: 0.0766. Time: 0.03\n",
      "2022-04-12 07:43:04,307 - INFO - train -   Epoch 0. Top1: 9.9081. Top5: 48.8787. best_acc: 9.9081 in epoch0\n",
      "2022-04-12 07:43:53,682 - INFO - train -   epoch:1, iter: 128. loss: 0.9992. loss_u: 0.1515. loss_x: 0.8477. Mask:16.7266 . LR: 0.1493. Time: 0.03\n",
      "2022-04-12 07:44:42,315 - INFO - train -   epoch:1, iter: 256. loss: 0.9430. loss_u: 0.1566. loss_x: 0.7864. Mask:18.1055 . LR: 0.1617. Time: 0.03\n",
      "2022-04-12 07:45:30,175 - INFO - train -   epoch:1, iter: 384. loss: 0.9059. loss_u: 0.1536. loss_x: 0.7523. Mask:19.0000 . LR: 0.1696. Time: 0.03\n",
      "2022-04-12 07:46:18,332 - INFO - train -   epoch:1, iter: 512. loss: 0.8749. loss_u: 0.1546. loss_x: 0.7204. Mask:20.2793 . LR: 0.1811. Time: 0.03\n",
      "2022-04-12 07:47:06,272 - INFO - train -   epoch:1, iter: 640. loss: 0.8559. loss_u: 0.1564. loss_x: 0.6994. Mask:21.3453 . LR: 0.1906. Time: 0.03\n",
      "2022-04-12 07:47:54,403 - INFO - train -   epoch:1, iter: 768. loss: 0.8365. loss_u: 0.1595. loss_x: 0.6771. Mask:22.6172 . LR: 0.2019. Time: 0.03\n",
      "2022-04-12 07:48:42,471 - INFO - train -   epoch:1, iter: 896. loss: 0.8163. loss_u: 0.1639. loss_x: 0.6524. Mask:23.8527 . LR: 0.2130. Time: 0.03\n",
      "2022-04-12 07:49:30,457 - INFO - train -   epoch:1, iter: 1024. loss: 0.7998. loss_u: 0.1708. loss_x: 0.6289. Mask:25.2676 . LR: 0.2256. Time: 0.03\n",
      "2022-04-12 07:49:34,529 - INFO - train -   Epoch 1. Top1: 35.8517. Top5: 69.6691. best_acc: 35.8517 in epoch1\n",
      "2022-04-12 07:50:23,687 - INFO - train -   epoch:2, iter: 128. loss: 0.6519. loss_u: 0.2039. loss_x: 0.4479. Mask:37.5000 . LR: 0.3348. Time: 0.03\n"
     ]
    }
   ],
   "source": [
    "model,criteria_x,criteria_u = create_model(args)\n",
    "num_iters_per_epoch = args.num_images_per_epoch // args.batch_size\n",
    "num_iters_all = num_iters_per_epoch * args.num_epoches\n",
    "ema = EMA(model,args.ema_decay)\n",
    "wd_params,non_wd_params = [],[]\n",
    "for name,param in model.named_parameters():\n",
    "    if 'bn' in name:\n",
    "        non_wd_params.append(param)\n",
    "    else:\n",
    "        wd_params.append(param)\n",
    "# print(len(wd_params),len(non_wd_params))\n",
    "param_list = [\n",
    "    {\n",
    "        'params':wd_params\n",
    "    },\n",
    "    {\n",
    "        'params':non_wd_params,\n",
    "        'weight_decay': 0\n",
    "    }\n",
    "]\n",
    "optimizer = torch.optim.SGD(param_list,lr=args.lr,weight_decay=args.wdecay,momentum=args.momentum,nesterov=args.nesterov)\n",
    "lr_schdlr = WarmupCosineLrScheduler(\n",
    "    optimizer,\n",
    "    max_iter=num_iters_all,\n",
    "    warmup_iter=0\n",
    ")\n",
    "\n",
    "\n",
    "best_acc = -1\n",
    "best_epoch = 0\n",
    "logger,writer = setup_default_logging(args)\n",
    "labeledDatasetDataloader,unlabeledDatasetDataloader=get_train_loader(args)\n",
    "validDatasetDataloader = get_valid_loader(args)\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Task = {args.dataset}@{args.num_labeled}\")\n",
    "logger.info(f\"  Num Epochs = {num_iters_per_epoch}\")\n",
    "logger.info(f\"  Batch size per GPU = {args.batch_size}\")\n",
    "# logger.info(f\"  Total train batch size = {args.batch_size * args.world_size}\")\n",
    "logger.info(f\"  Total optimization steps = {num_iters_all}\")\n",
    "logger.info(\"Total params: {:.2f}M\".format(\n",
    "    sum(p.numel() for p in model.parameters()) / 1e6))\n",
    "logger.info('-----------start training--------------')\n",
    "for epoch in range(args.num_epoches):\n",
    "    train_loss,loss_x,loss_u,mask_mean=train_one_epoch(\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        criteria_x=criteria_x,\n",
    "        criteria_u=criteria_u,\n",
    "        optimizer=optimizer,\n",
    "        lr_schdlr=lr_schdlr,\n",
    "        ema=ema,\n",
    "        labeledDatasetLoader=labeledDatasetDataloader,\n",
    "        unlabeledDatasetLoader=unlabeledDatasetDataloader,\n",
    "        lambda_u=args.lambda_u,\n",
    "        n_iters=num_iters_per_epoch,\n",
    "        args=args,\n",
    "        logger=logger \n",
    "    )\n",
    "    top1, top5, valid_loss = evaluate(ema, validDatasetDataloader, criteria_x, args=args)\n",
    "\n",
    "    writer.add_scalars('train/1.loss', {'train': train_loss,\n",
    "                                        'test': valid_loss}, epoch)\n",
    "    writer.add_scalar('train/2.train_loss_x', loss_x, epoch)\n",
    "    writer.add_scalar('train/3.train_loss_u', loss_u, epoch)\n",
    "    writer.add_scalar('train/4.mask_mean', mask_mean, epoch)\n",
    "    writer.add_scalars('test/1.test_acc', {'top1': top1, 'top5': top5}, epoch)\n",
    "    # writer.add_scalar('test/2.test_loss', loss, epoch)\n",
    "\n",
    "    # best_acc = top1 if best_acc < top1 else best_acc\n",
    "    if best_acc < top1:\n",
    "        best_acc = top1\n",
    "        best_epoch = epoch\n",
    "\n",
    "    logger.info(\"Epoch {}. Top1: {:.4f}. Top5: {:.4f}. best_acc: {:.4f} in epoch{}\".\n",
    "                format(epoch, top1, top5, best_acc, best_epoch))\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6b347f591e985303f98a5578330a51eea0838d27215b123c4c1d58108c23409"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
